<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3">
    <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
        <bitmap>media/images/box/root.png</bitmap>
        <script language="4">
            <content>
                <![CDATA[]]>
</content>
        </script>
        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
        <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
        <Timeline enable="0">
            <BehaviorLayer name="behavior_layer1">
                <BehaviorKeyframe name="keyframe1" index="1">
                    <Diagram>
                        <Box name="Face Reco." id="1" localization="8" tooltip="Detect people&apos;s face and recognize those which are known by the robot.&#x0A;&#x0A;Note: the robot needs to learn a face with the Learn Face box before he can recognize it." x="206" y="33">
                            <bitmap>media/images/box/interaction/reco_face.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.bIsRunning = False

    def onUnload(self):
        self.bIsRunning = False

    def onInput_onStart(self):
        self.bIsRunning = True

    def onInput_onStop(self):
        if( self.bIsRunning ):
            self.onUnload()
            self.onStopped()]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                            <Input name="FaceDetected" type="0" type_size="1" nature="4" stm_value_name="FaceDetected" inner="1" tooltip="Connected to ALMemory. Will be stimulated every time the value subscribed to changes, respecting the refresh period." id="4" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is stopped." id="5" />
                            <Output name="onRecognizedFaces" type="3" type_size="1" nature="2" inner="0" tooltip="Names of recognized faces. If several faces are recognized, they are sent one&#x0A;after an other on this output." id="6" />
                            <Timeline enable="0">
                                <BehaviorLayer name="behavior_layer1">
                                    <BehaviorKeyframe name="keyframe1" index="1">
                                        <Diagram>
                                            <Box name="Reco. Det. Faces" id="1" localization="8" tooltip="Process face detection extractor data (FaceDetected) to extract the labels of&#x0A;recognized faces and notify when there is a face detected but not recognized.&#x0A;&#x0A;An output (either one or the other) is stimulated each time the number of&#x0A;recognized faces change." x="185" y="44">
                                                <bitmap>media/images/box/interaction/reco_face.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[import qi
import random
class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)
        self.session = qi.Session()
        self.session.connect("192.168.1.105:9559")

        self.memory = self.session.service("ALMemory")
        self.facedetected = self.session.service("ALFaceDetection")
        self.tts = self.session.service("ALTextToSpeech")
        self.peopePerception = self.session.service("ALPeoplePerception")
        self.tabletService = self.session.service('ALTabletService')

    def onLoad(self):
        #self.tts.say("Im loading")
        self.timeFilteredResult = []
        self.signalID = 0
        self.counter = 0
        self.userNames = []


    def onUnload(self):
        #puts code for box cleanup here
        self.peopePerception.resetPopulation()


    def callback(self,int, value):

        if(value in self.userNames):
            self.tts.say("Thank you! hummmmmmmm! I have seen some one name" + value + " before!")

        # in case the button OK is press
        if int:

            done = self.facedetected.learnFace(value)
            if(done != true):
                self.facedetected.reLearnFace(value)
            self.logger.info("callback call" + value)
            self.memory.insertData("UserName",value)

        # in case the button CANCEL is press
        else :
            self.tts.say("Ok!buy buy")
            #self.onInput_onStop()
        #disconnect from the signal to avoid repetition
        self.tabletService.onInputText.disconnect(self.signalID)

    def onInput_onStart(self, p):
        if (self.counter > 0):

            self.onStopped()
        else:
            self.counter += 1
            #self.tts.say("A moment please!")
            if(len(p) > 0):
                if(len(p[1]) > 0): # just in case of the ALValue is in the wrong format


                    # get the ALValue returned by the time filtered recognition:
                    #    - [] when nothing new.
                    #    - [4] when a face has been detected but not recognized during the first 8s.
                    #    - [2, [faceName]] when one face has been recognized.
                    #    - [3, [faceName1, faceName2, ...]] when several faces have been recognized.

                    self.timeFilteredResult = p[1][len(p[1]) -1]
                    self.logger.info("I'm here")

                    if( len(self.timeFilteredResult) == 1 ):
                        self.logger.info("inside detectWithoutReco")
                        self.userNames = self.facedetected.getLearnedFacesList()
                        if(len(self.userNames) != 0):
                            self.tts.say("Sorry if i dont remenber you! Please Give me your name by fealing the space on my tablet")
                        else:
                            self.tts.say("Hello you! I a pepper!. i would like to know your name!please field the space shown on my tablet")
                        # If a face has been detected for more than 8s but not recognized
                        if(self.timeFilteredResult[0] == 4):
                            self.logger.info("Also here")
                            self.tabletService.showInputTextDialog("Enter your name please","OK","CANCEL","",10)
                            self.signalID = self.tabletService.onInputText.connect(self.callback)


                    elif ( len(self.timeFilteredResult) == 2 ):
                        if (self.timeFilteredResult[0] == 2):
                            self.logger.info("Finally here")
                            self.tts.say("well come back" + self.timeFilteredResult[1][0] + "i can see you have appreciate the game! let play one more" )
                            self.onRecognizedFace(self.timeFilteredResult[1][0])


    def onInput_onStop(self):
        pass]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                <Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input. It must be the&#x0A;FaceDetected extractor data." id="2" />
                                                <Input name="input" type="1" type_size="1" nature="3" inner="0" tooltip="" id="3" />
                                                <Output name="onRecognizedFace" type="3" type_size="1" nature="1" inner="0" tooltip="Name of recognized face. If several faces are recognized, they are sent one after an&#x0A;other on this output.&#x0A;&#x0A;Note: if it is the wrong face which has been recognized, you have 7s to rename&#x0A;it with the relearn function available on one of  Add/Del Faces sub-boxes (Learn Face&#x0A;box)." id="4" />
                                                <Output name="onDetectWithoutReco" type="1" type_size="1" nature="1" inner="0" tooltip="A face has been detected for more than 8s but has not been recognized. It means&#x0A;that the robot does not know this face but it would be a good idea to learn it." id="5" />
                                                <Output name="onStopped" type="1" type_size="1" nature="2" inner="0" tooltip="" id="6" />
                                            </Box>
                                            <Box name="Say Text" id="2" localization="8" tooltip="Say the text received on its input." x="378" y="21">
                                                <bitmap>media/images/box/interaction/say.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)
        self.tts = ALProxy('ALTextToSpeech')
        self.ttsStop = ALProxy('ALTextToSpeech', True) #Create another proxy as wait is blocking if audioout is remote

    def onLoad(self):
        self.bIsRunning = False
        self.ids = []

    def onUnload(self):
        for id in self.ids:
            try:
                self.ttsStop.stop(id)
            except:
                pass
        while( self.bIsRunning ):
            time.sleep( 0.2 )

    def onInput_onStart(self, p):
        self.bIsRunning = True
        try:
            sentence = "\RSPD="+ str( self.getParameter("Speed (%)") ) + "\ "
            sentence += "\VCT="+ str( self.getParameter("Voice shaping (%)") ) + "\ "
            sentence += str(p)
            sentence +=  "\RST\ "
            id = self.tts.post.say(str(sentence))
            self.ids.append(id)
            self.tts.wait(id, 0)
        finally:
            try:
                self.ids.remove(id)
            except:
                pass
            if( self.ids == [] ):
                self.onStopped() # activate output of the box
                self.bIsRunning = False

    def onInput_onStop(self):
        self.onUnload()]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
                                                <Input name="onStart" type="3" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this Input." id="2" />
                                                <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this Input." id="3" />
                                                <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when Box behavior is finished." id="4" />
                                                <Parameter name="Voice shaping (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="150" tooltip='Used to modify at runtime the voice feature (tone, speed). In a slighty&#x0A;different way than pitch and speed, it gives a kind of &quot;gender or age&#x0A;modification&quot; effect.&#x0A;&#x0A;For instance, a quite good male derivation of female voice can be&#x0A;obtained setting this parameter to 78%.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the&#x0A;speed parameter. For example, if you want to decrease by 20% the voice&#x0A;shaping, you will have to increase by 20% the speed to keep a constant&#x0A;average speed.' id="5" />
                                                <Parameter name="Speed (%)" inherits_from_parent="1" content_type="1" value="100" default_value="100" min="50" max="200" tooltip="Changes the speed of the voice.&#x0A;&#x0A;Note: For a better effect, you can compensate this parameter with the voice&#x0A;shaping parameter. For example, if you want to increase by 20% the speed, you&#x0A;will have to decrease by 20% the voice shaping to keep a constant average&#x0A;speed." id="6" />
                                                <Resource name="Speech" type="Lock" timeout="0" />
                                            </Box>
                                            <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="4" />
                                            <Link inputowner="1" indexofinput="3" outputowner="1" indexofoutput="6" />
                                            <Link inputowner="2" indexofinput="2" outputowner="1" indexofoutput="4" />
                                        </Diagram>
                                    </BehaviorKeyframe>
                                </BehaviorLayer>
                            </Timeline>
                        </Box>
                        <Box name="Unlearn Face" id="2" localization="8" tooltip="Unlearn the face which name is set on the input.&#x0A;&#x0A;Note: If the face has not been learnt before, the onFailure output will be stimulated.&#x0A;&#x0A;V1.1.0" x="378" y="210">
                            <bitmap>media/images/box/interaction/unlearn_face.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        #~ puts code for box initialization here
        self.blsRunning = False
        pass

    def onUnload(self):
        #~ puts code for box cleanup here
        pass

    def onInput_onUnlearn(self, p):
        #~ self.onSuccess() #~ activate output of the box
        if(self.blsRunning ):
            return
        self.blsRunning = True
        try:
            faceDetectionModule = ALProxy("ALFaceDetection")
        except Exception as e:
            faceDetectionModule = None
            self.logger.error(e)
        if( faceDetectionModule and faceDetectionModule.forgetPerson(p) ):
            self.onSuccess()
        else:
            self.onFailure()
        self.blsRunning = False]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onUnlearn" type="3" type_size="1" nature="2" inner="0" tooltip="Name of face you want the robot to unlearn." id="2" />
                            <Output name="onSuccess" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when the face has been successfully unlearnt." id="3" />
                            <Output name="onFailure" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when the box encountered an error (unknown face name, for example)." id="4" />
                        </Box>
                        <Box name="Text Edit" id="3" localization="8" tooltip="Send the text you entered when the input is stimulated." plugin="textedit_plugin" x="396" y="407">
                            <bitmap>media/images/box/interaction/vocabulary.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[# /!\ Generated content. Do not edit!
class MyClass(GeneratedClass):
	def __init__(self):
		GeneratedClass.__init__(self)

	def onInput_onStart(self):
		self.onStopped("roro")]]>
</content>
                            </script>
                            <pluginContent>
                                <text>
                                    <![CDATA[roro]]>
</text>
                            </pluginContent>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="1" inner="0" tooltip="To send the text on the output." id="2" />
                            <Output name="onStopped" type="3" type_size="1" nature="2" inner="0" tooltip="The text you entered." id="3" />
                        </Box>
                        <Box name="Basic Awareness" id="4" localization="8" tooltip="This box is an interface to the module ALBasicAwareness.&#x0A;&#x0A;It is a simple way to make the robot establish and keep eye contact with people.&#x0A;&#x0A;V1.1.0" x="141" y="155">
                            <bitmap>media/images/box/tracker/basicawareness.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)

    def onLoad(self):
        #put initialization code here
        try:
            self.awareness = ALProxy('ALBasicAwareness')
        except Exception as e:
            self.awareness = None
            self.logger.error(e)

        self.memory = ALProxy('ALMemory')

        self.isRunning = False
        self.trackedHuman = -1

        import threading
        self.subscribingLock = threading.Lock()

        self.BIND_PYTHON(self.getName(), "setParameter")


    def onUnload(self):
        if self.isRunning:
            if self.awareness:
                self.awareness.stopAwareness()
                self.setALMemorySubscription(False)
            self.isRunning = False


    def onInput_onStart(self):
        if self.isRunning:
            return # already running, nothing to do

        self.isRunning = True
        self.trackedHuman = -1
        if self.awareness:
            self.awareness.setEngagementMode(self.getParameter('Engagement Mode'))
            self.awareness.setTrackingMode(self.getParameter('Tracking Mode'))
            self.awareness.setStimulusDetectionEnabled('Sound', self.getParameter('Sound Stimulus'))
            self.awareness.setStimulusDetectionEnabled('Movement', self.getParameter('Movement Stimulus'))
            self.awareness.setStimulusDetectionEnabled('People', self.getParameter('People Stimulus'))
            self.awareness.setStimulusDetectionEnabled('Touch', self.getParameter('Touch Stimulus'))
            self.setALMemorySubscription(True)
            self.awareness.startAwareness()



    def onInput_onStop(self):
        if not self.isRunning:
            return # already stopped, nothing to do

        self.onUnload()
        self.onStopped()


    def setParameter(self, parameterName, newValue):
        GeneratedClass.setParameter(self, parameterName, newValue)

        if self.awareness:
            if parameterName == 'Sound Stimulus':
                self.awareness.setStimulusDetectionEnabled('Sound', newValue)
            elif parameterName == 'Movement Stimulus':
                self.awareness.setStimulusDetectionEnabled('Movement', newValue)
            elif parameterName == 'People Stimulus':
                self.awareness.setStimulusDetectionEnabled('People', newValue)
            elif parameterName == 'Touch Stimulus':
                self.awareness.setStimulusDetectionEnabled('Touch', newValue)


    # callbacks for ALBasicAwareness events
    def onStimulusDetected(self, eventName, stimulusName, subscriberIdentifier):
        self.StimulusDetected(stimulusName)

    def onHumanTracked(self, eventName, humanID, subscriberIdentifier):
        self.trackedHuman = humanID
        self.HumanTracked(humanID)

    def onHumanLost(self, eventName, subscriberIdentifier):
        self.HumanLost(self.trackedHuman)
        self.trackedHuman = -1


    def setALMemorySubscription(self, subscribe):
        self.subscribingLock.acquire()
        if subscribe:
            self.memory.subscribeToEvent('ALBasicAwareness/StimulusDetected', self.getName(), 'onStimulusDetected')
            self.memory.subscribeToEvent('ALBasicAwareness/HumanTracked', self.getName(), 'onHumanTracked')
            self.memory.subscribeToEvent('ALBasicAwareness/HumanLost', self.getName(), 'onHumanLost')
        else:
            self.memory.unsubscribeToEvent('ALBasicAwareness/StimulusDetected', self.getName())
            self.memory.unsubscribeToEvent('ALBasicAwareness/HumanTracked', self.getName())
            self.memory.unsubscribeToEvent('ALBasicAwareness/HumanLost', self.getName())

        self.subscribingLock.release()]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Starts the Basic Awareness with the given Engagement and Tracking mode parameters, using the given stimuli." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Stops the Basic Awareness." id="3" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                            <Output name="StimulusDetected" type="3" type_size="1" nature="2" inner="0" tooltip="This output is stimulated when BasicAwareness detects a stimulus amongst the tracked stimulus.&#x0A;&#x0A;The output data is the stimulus&apos; name." id="5" />
                            <Output name="HumanTracked" type="2" type_size="1" nature="2" inner="0" tooltip="This output is triggered when ALBasicAwareness detects a stimulus that is confirmed to be a human.&#x0A;&#x0A;The output data is the ID corresponding to the tracked human. It is shared with PeoplePerception and can be used there. This output is triggered with -1 if ALBasicAwareness tried to detect a human but failed." id="6" />
                            <Output name="HumanLost" type="2" type_size="1" nature="2" inner="0" tooltip="This output is triggered when the human currently tracked is lost.&#x0A;&#x0A; The output data is the ID corresponding to the lost human. It can be reused in PeoplePerception." id="7" />
                            <Parameter name="Engagement Mode" inherits_from_parent="0" content_type="3" value="FullyEngaged" default_value="Unengaged" custom_choice="0" tooltip='The engagement mode specifies how &quot;focused&quot; the robot is on the engaged person.' id="8">
                                <Choice value="Unengaged" />
                                <Choice value="FullyEngaged" />
                                <Choice value="SemiEngaged" />
                            </Parameter>
                            <Parameter name="Tracking Mode" inherits_from_parent="0" content_type="3" value="Head" default_value="Head" custom_choice="0" tooltip="The tracking mode describes how the robot keeps eye contact with an engaged person." id="9">
                                <Choice value="Head" />
                                <Choice value="BodyRotation" />
                                <Choice value="WholeBody" />
                            </Parameter>
                            <Parameter name="Sound Stimulus" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="10" />
                            <Parameter name="Movement Stimulus" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="11" />
                            <Parameter name="People Stimulus" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="12" />
                            <Parameter name="Touch Stimulus" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="13" />
                        </Box>
                        <Box name="LearnFaces" id="5" localization="8" tooltip="" x="200" y="433">
                            <bitmap>media/images/box/box-python-script.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.faces = ALProxy("ALFaceDetection")

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def onInput_onStart(self):
        #self.onStopped() #activate the output of the box
        faces = self.faces.getLearnedFacesList()
        self.logger.info(faces)

    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the box]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                        </Box>
                        <Link inputowner="2" indexofinput="2" outputowner="3" indexofoutput="3" />
                        <Link inputowner="4" indexofinput="2" outputowner="0" indexofoutput="2" />
                        <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" />
                    </Diagram>
                </BehaviorKeyframe>
            </BehaviorLayer>
        </Timeline>
    </Box>
</ChoregrapheProject>
